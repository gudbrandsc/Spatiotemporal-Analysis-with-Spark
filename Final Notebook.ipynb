{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Spatiotemporal Analysis with Spark (v 1.0)\n",
    "\n",
    "report by Gudbrand Schistad, Lovedeep Singh, Nate Wilson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 ms, sys: 4.36 ms, total: 17.9 ms\n",
      "Wall time: 358 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "import geohash\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/data/nam_tiny.tdv')\n",
    "df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "\n",
    "nam_tiny = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/data/nam_tiny.tdv')\n",
    "nam_1 = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/data/nam_s/nam_201501_s.tdv.gz')\n",
    "nam_s = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/data/nam_s/*')\n",
    "nam = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/data/nam/*')\n",
    "\n",
    "nam_tiny.createOrReplaceTempView(\"nam_tiny\")\n",
    "nam_1.createOrReplaceTempView(\"nam_1\")\n",
    "nam_s.createOrReplaceTempView(\"nam_s\")\n",
    "nam.createOrReplaceTempView(\"nam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown Feature: Choose a feature from the data dictionary above that you have never heard of before. Inspect some of the values for the feature (such as its average, min, max, etc.) and try to guess what it measures. Was your hypothesis correct? (Note: if you are a professional meteorologist, you can skip this question ;-))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 0.02586597390472889\n",
      "max: 0.8758659958839417\n",
      "avg: 0.3311159858293831\n",
      "CPU times: user 13.1 ms, sys: 6.78 ms, total: 19.9 ms\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "friction_velocity_surface_min = spark.sql(\"SELECT MIN(friction_velocity_surface) FROM TEMP_DF\").collect()\n",
    "friction_velocity_surface_max = spark.sql(\"SELECT MAX(friction_velocity_surface) FROM TEMP_DF\").collect()\n",
    "friction_velocity_surface_avg = spark.sql(\"SELECT AVG(friction_velocity_surface) FROM TEMP_DF\").collect()\n",
    "\n",
    "print(f\"min: {friction_velocity_surface_min[0][0]}\")\n",
    "print(f\"max: {friction_velocity_surface_max[0][0]}\")\n",
    "print(f\"avg: {friction_velocity_surface_avg[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose `friction_velocity_surface` as our feature of choice. We hypothesize that it is something to with airflow. \n",
    "\n",
    "The statistics support that hypothesis, given that I would expect the majority of places to have low airflow, but would also expect certain windy places, for example Chicago, the windy city, to have very high airflow. \n",
    "\n",
    "Wikipedia defines friction velocity as:\n",
    ">**Shear Velocity**, also called **friction velocity**, is a form by which a shear stress may be re-written in units of velocity. It is useful as a method in fluid mechanics to compare true velocities, such as the velocity of a flow in a stream, to a velocity that relates shear between layers of flow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot hot hot: When and where was the hottest temperature observed in the dataset? Is it an anomaly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(temperature_surface=306.4980163574219, (CAST(Timestamp AS DOUBLE) / CAST(1000 AS DOUBLE))=1426377600.0, Geohash='9qd23ynghrrz')]\n",
      "[Row(temperature_surface=306.4980163574219)]\n",
      "CPU times: user 9.5 ms, sys: 5.32 ms, total: 14.8 ms\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# warmup2\n",
    "maxtemp = spark.sql(   \"SELECT temperature_surface, Timestamp/1000, Geohash \\\n",
    "                        FROM TEMP_DF \\\n",
    "                        WHERE temperature_surface in \\\n",
    "                           (SELECT MAX(temperature_surface) \\\n",
    "                            FROM TEMP_DF)\").collect()\n",
    "\n",
    "\n",
    "all_other_temps_there = spark.sql(\n",
    "                      f\"SELECT DISTINCT temperature_surface\\\n",
    "                        FROM TEMP_DF \\\n",
    "                        WHERE Geohash = '{maxtemp[0][2]}'\").collect()\n",
    "print(maxtemp)\n",
    "print(all_other_temps_there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So Snowy: Find a location that is snowy all year (there are several). Locate a nearby town/city and provide a small writeup about it. Include pictures if you’d like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Geohash='f2w29r4werxb'), Row(Geohash='fccz22w4fytb'), Row(Geohash='c1nuq5290jup'), Row(Geohash='f2d5v1jeyp7z'), Row(Geohash='c6s64488ws80'), Row(Geohash='f2fh6jpdgv5b')]\n",
      "CPU times: user 4.38 ms, sys: 2.66 ms, total: 7.04 ms\n",
      "Wall time: 508 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# warmup 3\n",
    "snowysurface = spark.sql(  \"SELECT Geohash \\\n",
    "                            FROM TEMP_DF \\\n",
    "                            WHERE \\\n",
    "                                Geohash NOT IN \\\n",
    "                                   (SELECT distinct(Geohash) \\\n",
    "                                    FROM TEMP_DF \\\n",
    "                                    WHERE categorical_snow_yes1_no0_surface = 0)\").collect()\n",
    "print(snowysurface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strangely Snowy: Find a location that contains snow while its surroundings do not. Why does this occur? Is it a high mountain peak in a desert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.categorical_snow_yes1_no0_surface == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbor f2w29r4werx8\n",
      "[]\n",
      "0\n",
      "neighbor f2w29r4wex80\n",
      "[]\n",
      "0\n",
      "neighbor f2w29r4werx9\n",
      "[]\n",
      "0\n",
      "neighbor f2w29r4werxc\n",
      "[]\n",
      "0\n",
      "neighbor f2w29r4wex81\n",
      "[]\n",
      "0\n",
      "neighbor f2w29r4werrx\n",
      "[]\n",
      "0\n",
      "neighbor f2w29r4werrz\n",
      "[]\n",
      "0\n",
      "neighbor f2w29r4wex2p\n",
      "[]\n",
      "0\n",
      "strangely snowy place found\n",
      "cold place f2w29r4werxb\n",
      "neighbor fccz22w4fyt8\n",
      "[]\n",
      "0\n",
      "neighbor fccz22w4fyw0\n",
      "[]\n",
      "0\n",
      "neighbor fccz22w4fyt9\n",
      "[]\n",
      "0\n",
      "neighbor fccz22w4fytc\n",
      "[]\n",
      "0\n",
      "neighbor fccz22w4fyw1\n",
      "[]\n",
      "0\n",
      "neighbor fccz22w4fymx\n",
      "[]\n",
      "0\n",
      "neighbor fccz22w4fymz\n",
      "[]\n",
      "0\n",
      "neighbor fccz22w4fyqp\n",
      "[]\n",
      "0\n",
      "strangely snowy place found\n",
      "cold place fccz22w4fytb\n",
      "neighbor c1nuq5290jgz\n",
      "[]\n",
      "0\n",
      "neighbor c1nuq5290jur\n",
      "[]\n",
      "0\n",
      "neighbor c1nuq5290n5b\n",
      "[]\n",
      "0\n",
      "neighbor c1nuq5290nh0\n",
      "[]\n",
      "0\n",
      "neighbor c1nuq5290nh2\n",
      "[]\n",
      "0\n",
      "neighbor c1nuq5290jgy\n",
      "[]\n",
      "0\n",
      "neighbor c1nuq5290jun\n",
      "[]\n",
      "0\n",
      "neighbor c1nuq5290juq\n",
      "[]\n",
      "0\n",
      "strangely snowy place found\n",
      "cold place c1nuq5290jup\n",
      "neighbor f2d5v1jeyp7x\n",
      "[]\n",
      "0\n",
      "neighbor f2d5v1jeypkp\n",
      "[]\n",
      "0\n",
      "neighbor f2d5v1jeype8\n",
      "[]\n",
      "0\n",
      "neighbor f2d5v1jeypeb\n",
      "[]\n",
      "0\n",
      "neighbor f2d5v1jeyps0\n",
      "[]\n",
      "0\n",
      "neighbor f2d5v1jeyp7w\n",
      "[]\n",
      "0\n",
      "neighbor f2d5v1jeyp7y\n",
      "[]\n",
      "0\n",
      "neighbor f2d5v1jeypkn\n",
      "[]\n",
      "0\n",
      "strangely snowy place found\n",
      "cold place f2d5v1jeyp7z\n",
      "neighbor c6s64488wkxb\n",
      "[]\n",
      "0\n",
      "neighbor c6s64488ws82\n",
      "[]\n",
      "0\n",
      "neighbor c6s64488wkxc\n",
      "[]\n",
      "0\n",
      "neighbor c6s64488ws81\n",
      "[]\n",
      "0\n",
      "neighbor c6s64488ws83\n",
      "[]\n",
      "0\n",
      "neighbor c6s64488wkrz\n",
      "[]\n",
      "0\n",
      "neighbor c6s64488ws2p\n",
      "[]\n",
      "0\n",
      "neighbor c6s64488ws2r\n",
      "[]\n",
      "0\n",
      "strangely snowy place found\n",
      "cold place c6s64488ws80\n",
      "neighbor f2fh6jpdgv58\n",
      "[]\n",
      "0\n",
      "neighbor f2fh6jpdgvh0\n",
      "[]\n",
      "0\n",
      "neighbor f2fh6jpdgv59\n",
      "[]\n",
      "0\n",
      "neighbor f2fh6jpdgv5c\n",
      "[]\n",
      "0\n",
      "neighbor f2fh6jpdgvh1\n",
      "[]\n",
      "0\n",
      "neighbor f2fh6jpdgugx\n",
      "[]\n",
      "0\n",
      "neighbor f2fh6jpdgugz\n",
      "[]\n",
      "0\n",
      "neighbor f2fh6jpdguup\n",
      "[]\n",
      "0\n",
      "strangely snowy place found\n",
      "cold place f2fh6jpdgv5b\n",
      "CPU times: user 210 ms, sys: 30.1 ms, total: 240 ms\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO\n",
    "# TODO\n",
    "# TODO : THIS DOESNT WORK\n",
    "snow_covered_locations = df[df.categorical_snow_yes1_no0_surface == 1].collect()\n",
    "\n",
    "for row in snow_covered_locations:\n",
    "    is_snowy = False\n",
    "    neighbors = geohash.neighbors(row.Geohash)\n",
    "    for neighbor in neighbors:\n",
    "        print(\"neighbor \" + neighbor)\n",
    "        count = spark.sql(f\"SELECT count(*) as cc \\\n",
    "                            FROM TEMP_DF \\\n",
    "                            WHERE \\\n",
    "                                Geohash = '{neighbor}'\\\n",
    "                                AND categorical_snow_yes1_no0_surface = 1 \\\n",
    "                            HAVING count(*) > 0\").collect()\n",
    "        print(count)\n",
    "        print(len(count))\n",
    "        if(len(count) > 0):\n",
    "            print('neighbor snowy :(')\n",
    "            issnowy = True\n",
    "            break\n",
    "    \n",
    "    if (is_snowy == False):\n",
    "        print(\"strangely snowy place found\")\n",
    "        print(\"cold place \" + row.Geohash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning rod: Where are you most likely to be struck by lightning? Use a precision of at least 4 Geohash characters and provide the top 3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drying out: Choose a region in North America (defined by one or more Geohashes) and determine when its driest month is. This should include a histogram with data from each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Startup: After graduating from USF, you found a startup that aims to provide personalized travel itineraries using big data analysis. Given your own personal preferences, build a plan for a year of travel across 5 locations. Or, in other words: pick 5 regions. What is the best time of year to visit them based on the dataset? One avenue here could be determining the comfort index for a region. You could incorporate several features: not too hot, not too cold, dry, humid, windy, etc. There are several different ways of calculating this available online, and you could also analyze how well your own metrics do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# Travel Startup\n",
    "relative_humidity_zerodegc_isotherm = 45\n",
    "min_surface_wind_gust_surface = 1.67\n",
    "max_surface_wind_gust_surface = 3.1\n",
    "temperature_surface = 77\n",
    "total_cloud_cover_entire_atmosphere = 60 \n",
    "# Grand canyon 9qrhf6btbt3jevhn\n",
    "# Panhandle san francisco 9q8yvs4t\n",
    "# New york dr5regw2z6y\n",
    "# Fresno 9qd23ynghrrz\n",
    "\n",
    "traveldata = spark.sql( \"SELECT  \\\n",
    "                            substring(Geohash, 0, 5) as region, \\\n",
    "                            MONTH(FROM_UNIXTIME(Timestamp/1000)) as month, \\\n",
    "                            AVG(temperature_surface) as temperature, \\\n",
    "                            AVG(relative_humidity_zerodegc_isotherm) as humdity, \\\n",
    "                            AVG(surface_wind_gust_surface) as windspeed, \\\n",
    "                            AVG(total_cloud_cover_entire_atmosphere) as cloudcover \\\n",
    "                        FROM nam_s \\\n",
    "                        WHERE \\\n",
    "                            Geohash like '9qd23%' or \\\n",
    "                            Geohash like 'dr5re%' or \\\n",
    "                            Geohash like '9q8yv%' or \\\n",
    "                            Geohash like '9qrhf%'  \\\n",
    "                        GROUP BY \\\n",
    "                            MONTH(FROM_UNIXTIME(Timestamp/1000)), \\\n",
    "                            substring(Geohash, 0, 5)\").collect()\n",
    "for row in traveldata:\n",
    "    isgood = True\n",
    "    if(row.temperature < (temperature_surface - (temperature_surface * .1)) or  \n",
    "       row.temperature > (temperature_surface + (temperature_surface * .1))):\n",
    "        isgood = False\n",
    "    if(row.humdity < (relative_humidity_zerodegc_isotherm - (relative_humidity_zerodegc_isotherm * .1)) or  \n",
    "       row.humdity > (relative_humidity_zerodegc_isotherm + (relative_humidity_zerodegc_isotherm * .1))):\n",
    "        isgood = False\n",
    "    if(row.windspeed < min_surface_wind_gust_surface or  \n",
    "       row.windspeed > max_surface_wind_gust_surface):\n",
    "        isgood = False\n",
    "    if(row.cloudcover < (total_cloud_cover_entire_atmosphere - (total_cloud_cover_entire_atmosphere * .1)) or  \n",
    "       row.cloudcover > (total_cloud_cover_entire_atmosphere + (total_cloud_cover_entire_atmosphere * .1))):\n",
    "        isgood = False\n",
    "    if(isgood == True):\n",
    "        print(f\"{row.region} {row.month}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escaping the fog: After becoming rich from your startup, you are looking for the perfect location to build your Bay Area mansion with unobstructed views. Find the locations that are the least foggy and show them on a map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SolarWind, Inc.: You get bored enjoying the amazing views from your mansion, so you start a new company; here, you want to help power companies plan out the locations of solar and wind farms across North America. Locate the top 3 places for solar and wind farms, as well as a combination of both (solar + wind farm). You will report a total of 9 Geohashes as well as their relevant attributes (for example, cloud cover and wind speeds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate Chart: Given a Geohash prefix, create a climate chart for the region. This includes high, low, and average temperatures, as well as monthly average rainfall (precipitation). Here’s a (poor quality) script that will generate this for you.\n",
    "Earn up to 1 point of extra credit for enhancing/improving this chart (or porting it to a more feature-rich visualization library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influencers: Determine how features influence each other using Pearson’s correlation coefficient (PCC). The output for this job should include (1) feature pairs sorted by absolute correlation coefficient, and (2) a correlation matrix visualization (heatmaps are a good option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql('SELECT \\\n",
    "`Timestamp`, \\\n",
    "`geopotential_height_lltw`, \\\n",
    "`water_equiv_of_accum_snow_depth_surface`, \\\n",
    "`drag_coefficient_surface`, \\\n",
    "`sensible_heat_net_flux_surface`, \\\n",
    "`categorical_ice_pellets_yes1_no0_surface`, \\\n",
    "`visibility_surface`, \\\n",
    "`number_of_soil_layers_in_root_zone_surface`, \\\n",
    "`categorical_freezing_rain_yes1_no0_surface`, \\\n",
    "`pressure_reduced_to_msl_msl`, \\\n",
    "`upward_short_wave_rad_flux_surface`, \\\n",
    "`relative_humidity_zerodegc_isotherm`, \\\n",
    "`categorical_snow_yes1_no0_surface`, \\\n",
    "`u-component_of_wind_tropopause`, \\\n",
    "`surface_wind_gust_surface`, \\\n",
    "`total_cloud_cover_entire_atmosphere`, \\\n",
    "`upward_long_wave_rad_flux_surface`, \\\n",
    "`land_cover_land1_sea0_surface`, \\\n",
    "`vegitation_type_as_in_sib_surface`, \\\n",
    "`v-component_of_wind_pblri`, \\\n",
    "`albedo_surface`, \\\n",
    "`lightning_surface`, \\\n",
    "`ice_cover_ice1_no_ice0_surface`, \\\n",
    "`convective_inhibition_surface`, \\\n",
    "`pressure_surface`, \\\n",
    "`transpiration_stress-onset_soil_moisture_surface`, \\\n",
    "`soil_porosity_surface`, \\\n",
    "`vegetation_surface`, \\\n",
    "`categorical_rain_yes1_no0_surface`, \\\n",
    "`downward_long_wave_rad_flux_surface`, \\\n",
    "`planetary_boundary_layer_height_surface`, \\\n",
    "`soil_type_as_in_zobler_surface`, \\\n",
    "`geopotential_height_cloud_base`, \\\n",
    "`friction_velocity_surface`, \\\n",
    "`maximumcomposite_radar_reflectivity_entire_atmosphere`, \\\n",
    "`plant_canopy_surface_water_surface`, \\\n",
    "`v-component_of_wind_maximum_wind`, \\\n",
    "`geopotential_height_zerodegc_isotherm`, \\\n",
    "`mean_sea_level_pressure_nam_model_reduction_msl`, \\\n",
    "`temperature_surface`, \\\n",
    "`snow_cover_surface`, \\\n",
    "`geopotential_height_surface`, \\\n",
    "`convective_available_potential_energy_surface`, \\\n",
    "`latent_heat_net_flux_surface`, \\\n",
    "`surface_roughness_surface`, \\\n",
    "`pressure_maximum_wind`, \\\n",
    "`temperature_tropopause`, \\\n",
    "`geopotential_height_pblri`, \\\n",
    "`pressure_tropopause`, \\\n",
    "`snow_depth_surface`, \\\n",
    "`v-component_of_wind_tropopause`, \\\n",
    "`downward_short_wave_rad_flux_surface`, \\\n",
    "`u-component_of_wind_maximum_wind`, \\\n",
    "`wilting_point_surface`, \\\n",
    "`precipitable_water_entire_atmosphere`, \\\n",
    "`u-component_of_wind_pblri`, \\\n",
    "`direct_evaporation_cease_soil_moisture_surface` FROM nam_tiny')\n",
    "\n",
    "features = df.rdd.map(lambda row: row[0:])\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Arial']})\n",
    "\n",
    "plt.suptitle('Correlation Heatmap', fontsize=16)\n",
    "plt.xlabel('Dimension ID', fontsize=14)\n",
    "plt.ylabel('Dimension ID', fontsize=14)\n",
    "\n",
    "plt.pcolor(corr_mat, cmap='RdBu_r')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Correlation Coefficient', fontsize=14)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction/Classification: Using what you learned above as your guide, choose a feature to predict or classify via machine learning models in MLlib. You will need to explain: (1) The feature you will predict/classify (2) Features used to train the model (3) How you partitioned your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Machine learning\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def prepare_data(dframe, predictors, target):\n",
    "    assembler = VectorAssembler(inputCols=predictors, outputCol=\"features\")\n",
    "    output = assembler.transform(dframe)\n",
    "    return output.select(\"features\", target).withColumnRenamed(target, \"label\")\n",
    "\n",
    "\n",
    "prepped = prepare_data(df,\n",
    "    [\"geopotential_height_lltw\", \n",
    "     \"upward_long_wave_rad_flux_surface\", \n",
    "     \"albedo_surface\", \n",
    "     \"downward_long_wave_rad_flux_surface\",\n",
    "     \"plant_canopy_surface_water_surface\",\n",
    "     \"geopotential_height_zerodegc_isotherm\",\n",
    "     \"temperature_surface\",\n",
    "     \"snow_depth_surface\"\n",
    "    ],\n",
    "    \"snow_cover_surface\")\n",
    "\n",
    "prepped.show()\n",
    "(trainingData, testData) = prepped.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_df = spark.read.csv('hdfs://orion11:25000/train_2v.csv', inferSchema=True,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the types of insights you hope to gain from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make hypotheses about what you might find\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design at least 3 “questions” (along the lines of those above) and answer them. Remember that presentation matters here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: int, gender: string, age: double, hypertension: int, heart_disease: int, ever_married: string, work_type: string, Residence_type: string, avg_glucose_level: double, bmi: double, smoking_status: string, stroke: int]\n",
      "[Row(stroke=1), Row(stroke=0)]\n",
      "CPU times: user 3.13 ms, sys: 1.91 ms, total: 5.04 ms\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aa_df.createOrReplaceTempView(\"aa_df\")\n",
    "\n",
    "print(aadd)\n",
    "\n",
    "stroke = spark.sql(\"SELECT distinct(stroke) FROM aa_df\").collect()\n",
    "print(stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|    work_type|stroke_patients|\n",
      "+-------------+---------------+\n",
      "|      Private|            441|\n",
      "|Self-employed|            251|\n",
      "|     Govt_job|             89|\n",
      "|     children|              2|\n",
      "+-------------+---------------+\n",
      "\n",
      "CPU times: user 1.64 ms, sys: 360 µs, total: 2 ms\n",
      "Wall time: 565 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Find the occupation which is most vulernale to the stroke\n",
    "spark.sql( \"SELECT work_type, count(*) as stroke_patients \\\n",
    "            FROM aa_df \\\n",
    "            WHERE stroke = 1 \\\n",
    "            GROUP BY work_type \\\n",
    "            ORDER BY count(*) desc\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|gender|count|            percent|\n",
      "+------+-----+-------------------+\n",
      "|Female|25665|  59.13594470046083|\n",
      "| Other|   11|0.02534562211981567|\n",
      "|  Male|17724|  40.83870967741935|\n",
      "+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Factoring affecting the stroke in the people\n",
    "spark.sql( \"SELECT \\\n",
    "                gender, \\\n",
    "                count(gender) as count, \\\n",
    "                count(gender)*100/sum(count(gender)) over() as percent \\\n",
    "            FROM aa_df \\\n",
    "            GROUP BY gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|age_range|count|\n",
      "+---------+-----+\n",
      "|      >50|17202|\n",
      "|    26-50|14652|\n",
      "|     0-25|11546|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Age as factor\n",
    "spark.sql(\"SELECT case when age <= 25 then '0-25' else case when age <=50 then '26-50' else '>50' end end as age_range, \\\n",
    "           count(*) as count \\\n",
    "           FROM aa_df \\\n",
    "           group by 1 order by count desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values\n",
    "train_f = aa_df.na.fill('No Info', subset=['smoking_status'])\n",
    "# fill in miss values with mean\n",
    "from pyspark.sql.functions import mean\n",
    "mean = train_f.select(mean(train_f['bmi'])).collect()\n",
    "mean_bmi = mean[0][0]\n",
    "train_f = train_f.na.fill(mean_bmi,['bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,OneHotEncoder,\n",
    "                                StringIndexer)\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='gender', outputCol = 'genderIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='genderIndex', outputCol = 'genderVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ever_married_indexer = StringIndexer(inputCol='ever_married', outputCol = 'ever_marriedIndex')\n",
    "ever_married_encoder = OneHotEncoder(inputCol='ever_marriedIndex', outputCol = 'ever_marriedVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_type_indexer = StringIndexer(inputCol='work_type', outputCol = 'work_typeIndex')\n",
    "work_type_encoder = OneHotEncoder(inputCol='work_typeIndex', outputCol = 'work_typeVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Residence_type_indexer = StringIndexer(inputCol='Residence_type', outputCol = 'Residence_typeIndex')\n",
    "Residence_type_encoder = OneHotEncoder(inputCol='Residence_typeIndex', outputCol = 'Residence_typeVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking_status_indexer = StringIndexer(inputCol='smoking_status', outputCol = 'smoking_statusIndex')\n",
    "smoking_status_encoder = OneHotEncoder(inputCol='smoking_statusIndex', outputCol = 'smoking_statusVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['genderVec',\n",
    " 'age',\n",
    " 'hypertension',\n",
    " 'heart_disease',\n",
    " 'ever_marriedVec',\n",
    " 'work_typeVec',\n",
    " 'Residence_typeVec',\n",
    " 'avg_glucose_level',\n",
    " 'bmi',\n",
    " 'smoking_statusVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='stroke',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer, ever_married_indexer, work_type_indexer, Residence_type_indexer,\n",
    "                           smoking_status_indexer, gender_encoder, ever_married_encoder, work_type_encoder,\n",
    "                           Residence_type_encoder, smoking_status_encoder, assembler, dtc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = train_f.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using decision tree is: 98.21%\n"
     ]
    }
   ],
   "source": [
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "print('Accuracy using decision tree is: {0:2.2f}%'.format(dtc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
